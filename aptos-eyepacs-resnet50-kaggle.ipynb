{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/AlbertoRomanRamosRodriguez/Tesis-de-Grado/blob/main/APTOS_EYEPACS_grading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"v3U8oUzLg8Ng"},"source":["# Grading Diabetic Retinopath with APTOS and EYEPACS"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:42.734349Z","iopub.status.busy":"2023-05-14T00:11:42.733389Z","iopub.status.idle":"2023-05-14T00:11:42.768538Z","shell.execute_reply":"2023-05-14T00:11:42.767580Z","shell.execute_reply.started":"2023-05-14T00:11:42.734307Z"},"id":"riAvpykNkyt5","outputId":"b4168b5f-37ea-466f-dcd3-17bfabee88e5","trusted":true},"outputs":[],"source":["# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:42.771215Z","iopub.status.busy":"2023-05-14T00:11:42.770842Z","iopub.status.idle":"2023-05-14T00:11:44.474978Z","shell.execute_reply":"2023-05-14T00:11:44.473971Z","shell.execute_reply.started":"2023-05-14T00:11:42.771176Z"},"id":"3RloYXqYjbY6","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from shutil import copy, unpack_archive,move\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:44.484292Z","iopub.status.busy":"2023-05-14T00:11:44.481240Z","iopub.status.idle":"2023-05-14T00:11:44.620982Z","shell.execute_reply":"2023-05-14T00:11:44.619861Z","shell.execute_reply.started":"2023-05-14T00:11:44.484213Z"},"id":"fNbBrfmEuRGj","outputId":"3b862192-a94b-463a-96dc-96219f1ed667","trusted":true},"outputs":[],"source":["aptos_csv = '/kaggle/input/aptos2019-blindness-detection/train.csv'\n","eyepacs_csv = '/kaggle/input/eyepacspreprocess/trainLabels.csv'\n","dataframes = [pd.read_csv(csv) for csv in [aptos_csv, eyepacs_csv]]\n","\n","name_mappings = {\n","    'id_code': 'image',\n","    'diagnosis': 'level'\n","}\n","\n","\n","for d, extension in zip(dataframes, ['.png', '.jpeg']):\n","  d.rename(columns=name_mappings, inplace=True)\n","  d['image'] = d['image']+ extension\n","  display(d.head())\n","\n","bd_names = ('APTOS 19', 'EYEPACS')\n","\n","classes = range(5)\n","orig_ds = pd.concat([dfn.assign(DB=bdn) for dfn, bdn in zip(dataframes, bd_names)])\n","orig_ds.reset_index(inplace=True)\n","orig_ds.drop(columns=['index'], inplace=True)\n","orig_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:44.629980Z","iopub.status.busy":"2023-05-14T00:11:44.627275Z","iopub.status.idle":"2023-05-14T00:11:45.288046Z","shell.execute_reply":"2023-05-14T00:11:45.282576Z","shell.execute_reply.started":"2023-05-14T00:11:44.629936Z"},"trusted":true},"outputs":[],"source":["# Create the bar plot\n","sns.countplot(x='level', data=orig_ds, hue='DB')\n","\n","# Set the plot title and axis labels\n","plt.title('Cantidad de imágenes por grado de DR')\n","plt.xlabel('Grado')\n","plt.ylabel('Cantidad de imágenes')\n","\n","# Show the plot\n","plt.savefig('distribution.jpg')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:45.295729Z","iopub.status.busy":"2023-05-14T00:11:45.289918Z","iopub.status.idle":"2023-05-14T00:11:45.934169Z","shell.execute_reply":"2023-05-14T00:11:45.932313Z","shell.execute_reply.started":"2023-05-14T00:11:45.295670Z"},"id":"og8MRJAQ1AxO","outputId":"f7ffc3eb-a163-4e23-e573-b95f830baff2","trusted":true},"outputs":[],"source":["def reset_path(directory:str):\n","    \"\"\"\n","    Deletes an existing directory and re-creates it\n","\n","    - directory: path to directory\n","    \"\"\"\n","    if os.path.exists(directory):\n","        !rm -r {directory}\n","    os.mkdir(directory)\n","\n","DB_PATH = './DB'\n","reset_path(DB_PATH)\n","\n","display(orig_ds.groupby('level')['image'].count())\n","\n","classes = 5\n","plt.pie([orig_ds.loc[orig_ds['level'] == n].count()[0] for n in range(classes)], labels = range(classes), autopct='%.2f %%')\n","plt.title(\"Levels of DR in the original dataset\")\n","img_path = os.path.sep.join([DB_PATH, 'distribution-pie.jpg'])\n","plt.savefig(img_path)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:45.948228Z","iopub.status.busy":"2023-05-14T00:11:45.945129Z","iopub.status.idle":"2023-05-14T00:11:46.289644Z","shell.execute_reply":"2023-05-14T00:11:46.288453Z","shell.execute_reply.started":"2023-05-14T00:11:45.948181Z"},"id":"kmBM8iT92MMV","outputId":"28b72476-8cef-4aa3-fdae-2f8097d0e83a","trusted":true},"outputs":[],"source":["dataset = orig_ds.groupby(['level']).sample(1000)\n","display(dataset.groupby('level')['image'].count())\n","\n","plt.pie([dataset.loc[dataset['level'] == n].count()[0] for n in range(classes)], labels = range(classes), autopct='%.2f %%')\n","plt.title(\"Levels of DR in the redistributed dataset\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:46.292570Z","iopub.status.busy":"2023-05-14T00:11:46.292141Z","iopub.status.idle":"2023-05-14T00:11:46.421109Z","shell.execute_reply":"2023-05-14T00:11:46.420044Z","shell.execute_reply.started":"2023-05-14T00:11:46.292529Z"},"id":"jQqyfvLf2gZB","outputId":"fdd91270-9fbf-4bc6-cf73-63534235ff5c","trusted":true},"outputs":[],"source":["x_train, x_remain, y_train, y_remain = train_test_split(\n","    dataset['image'],\n","    dataset['level'],\n","    train_size=0.8,\n","    test_size=0.2,\n","    random_state=42\n",")\n","\n","train_df = pd.DataFrame()\n","train_df['image'] = x_train\n","train_df['level'] = y_train\n","\n","remain_df = pd.DataFrame()\n","remain_df['image'] = x_remain\n","remain_df['level'] = y_remain\n","\n","x_val, x_test, y_val, y_test = train_test_split(\n","    remain_df['image'],\n","    remain_df['level'],\n","    train_size=0.5,\n","    test_size=0.5,\n","    random_state=42\n",")\n","\n","val_df = pd.DataFrame()\n","val_df['image'] = x_val\n","val_df['level'] = y_val\n","\n","test_df = pd.DataFrame()\n","test_df['image'] = x_test\n","test_df['level'] = y_test\n","\n","display(train_df.groupby(['level'])['image'].count())\n","display(train_df.head())\n","display(val_df.groupby(['level'])['image'].count())\n","display(val_df.head())\n","display(test_df.groupby(['level'])['image'].count())\n","display(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:46.428096Z","iopub.status.busy":"2023-05-14T00:11:46.425636Z","iopub.status.idle":"2023-05-14T00:11:47.741545Z","shell.execute_reply":"2023-05-14T00:11:47.740308Z","shell.execute_reply.started":"2023-05-14T00:11:46.428055Z"},"id":"Z3ydKoIm9EOT","outputId":"55a67b1b-c301-4d76-9e17-33477fa16ebb","trusted":true},"outputs":[],"source":["path_dict = {p:os.listdir(p)\n","              for p in ['/kaggle/input/aptos2019-blindness-detection/train_images', '/kaggle/input/eyepacspreprocess/eyepacs_preprocess/eyepacs_preprocess']}\n","path_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:11:47.744224Z","iopub.status.busy":"2023-05-14T00:11:47.743097Z","iopub.status.idle":"2023-05-14T00:11:47.752331Z","shell.execute_reply":"2023-05-14T00:11:47.751021Z","shell.execute_reply.started":"2023-05-14T00:11:47.744181Z"},"id":"ANqCpEjvDYAW","trusted":true},"outputs":[],"source":["def clahe_image(path:str):\n","    # Load the image\n","    img = cv2.imread(path)\n","    # mask = cv2.imread('mask.png', 0)\n","\n","    # Extract the green plane\n","    green_plane = img[:, :, 1]\n","\n","    # Define the FA-CLAHE algorithm\n","    faclahe = cv2.createCLAHE(clipLimit=4.5, tileGridSize=(22, 22))\n","\n","    # Apply the FA-CLAHE algorithm to the green plane\n","    faclahe_img = faclahe.apply(green_plane)\n","\n","    # masked_img = cv2.bitwise_and(faclahe_img, faclahe_img, mask=mask)\n","\n","    cv2.imwrite(path, faclahe_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2023-05-14T00:11:47.760223Z","iopub.status.busy":"2023-05-14T00:11:47.759785Z","iopub.status.idle":"2023-05-14T00:18:55.128689Z","shell.execute_reply":"2023-05-14T00:18:55.127474Z","shell.execute_reply.started":"2023-05-14T00:11:47.760180Z"},"id":"treuYm_V3dBS","outputId":"1ba7a65e-06dd-4387-a8e4-aa231895d090","scrolled":true,"trusted":true},"outputs":[],"source":["from shutil import copy\n","import cv2\n","\n","splits = [\n","    (train_df, 'train'),\n","    (val_df, 'val'),\n","    (test_df, 'test')\n","]\n","\n","for df, set_path in splits:\n","  df['level'] = df['level'].astype('str')\n","  ds_dict = df.to_dict(orient='index')\n","  set_path = os.path.join(DB_PATH, set_path)\n","  if not os.path.exists(set_path):\n","    os.mkdir(set_path)\n","\n","  for datapoint in ds_dict.values():\n","    orig_path = ''\n","    for p, imgs in path_dict.items():\n","        orig_path = os.path.join(p, datapoint['image']) if datapoint['image'] in imgs else orig_path\n","\n","    if orig_path == '':\n","      raise ValueError\n","    dest_path = os.path.join(set_path, datapoint['level'])\n","    \n","    if not os.path.exists(dest_path):\n","      os.mkdir(dest_path)\n","    \n","    dest_path = os.path.join(dest_path, datapoint['image'])\n","\n","    copy(orig_path, dest_path)\n","    clahe_image(dest_path)\n","\n","    print(f'Copied {dest_path}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:18:55.130927Z","iopub.status.busy":"2023-05-14T00:18:55.130521Z","iopub.status.idle":"2023-05-14T00:18:56.375766Z","shell.execute_reply":"2023-05-14T00:18:56.374580Z","shell.execute_reply.started":"2023-05-14T00:18:55.130875Z"},"id":"r-eqocNVPmy3","outputId":"8c734087-065f-48cf-ac48-c06b2c5cf3ae","trusted":true},"outputs":[],"source":["# ResNet Grading of Diabetic Retinopathy in PyTorch\n","from __future__ import print_function, division\n","\n","from torchvision import datasets, models, transforms\n","from sklearn.metrics import confusion_matrix\n","import torch.backends.cudnn as cudnn\n","from torch.optim import lr_scheduler\n","\n","import matplotlib.pyplot as plt\n","import torch.optim as optim\n","import torch.nn as nn\n","import seaborn as sn\n","import pandas as pd\n","import numpy as np\n","import torchvision\n","import torch\n","import time\n","import copy\n","import os\n","\n","cudnn.benchmark = True\n","plt.ion()   # interactive mode\n","\n","## Training the model\n","def train_model(model, criterion, optimizer, scheduler, num_epochs=25, starting_loss = float('inf'), warmup_epochs = 0):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = starting_loss\n","    best_acc = 0.0 \n","    \n","    total_epochs = num_epochs+warmup_epochs\n","    try:\n","        for epoch in range(total_epochs):\n","            print(f'Epoch {epoch+1}/{total_epochs}')\n","            \n","            if warmup_epochs != 0 and epoch == (warmup_epochs - 1):\n","                for param in model_ft.parameters():\n","                    param.requires_grad = True\n","                print(f\"Starting full training\")\n","            elif warmup_epochs != 0 and epoch <= (warmup_epochs - 1):\n","                print(f\"{warmup_epochs-epoch+1} warmup epochs remaining\")\n","            elif warmup_epochs != 0 and epoch == 0:\n","                print(f\"Starting warmup training\")\n","            elif epoch == 0:\n","                print(\"Training without warmup\")\n","            print('-' * 10)\n","\n","            # Each epoch has a training and validation phase\n","            for phase in ['train', 'val']:\n","                if phase == 'train':\n","                    model.train()  # Set model to training mode\n","                else:\n","                    model.eval()   # Set model to evaluate mode\n","\n","                running_loss = 0.0\n","                running_corrects = 0\n","\n","                # Iterate over data\n","                for inputs, labels in dataloaders[phase]:\n","                    inputs = inputs.to(device)\n","                    labels = labels.to(device)\n","\n","                    # Zero the parameter gradients\n","                    optimizer.zero_grad()\n","\n","                    # Forward\n","                    # Track history if only in train\n","                    with torch.set_grad_enabled(phase == 'train'):\n","                        outputs = model(inputs)\n","                        _, preds = torch.max(outputs, 1)\n","                        loss = criterion(outputs, labels)\n","\n","                        # Backward + optimize only if in training phase\n","                        if phase == 'train':\n","                            loss.backward()\n","                            optimizer.step()\n","\n","                    # Statistics\n","                    running_loss += loss.item() * inputs.size(0)\n","                    running_corrects += torch.sum(preds == labels.data)\n","\n","                if phase == 'train':\n","                    scheduler.step()\n","\n","                epoch_loss = running_loss / dataset_sizes[phase]\n","                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","\n","                # Update best validation loss and model weights\n","                if phase == 'val' and epoch_loss < best_loss:\n","                    best_loss = epoch_loss\n","                    best_acc = epoch_acc\n","                    best_model_wts = copy.deepcopy(model.state_dict())\n","                    print(f'New best model found with validation loss: {best_loss:.4f}')\n","\n","            print()\n","\n","    except KeyboardInterrupt:\n","        pass\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best validation loss: {best_loss:.4f} | Best validation accuracy: {best_acc:.4f}')\n","\n","    # Load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return (best_loss, model)\n","\n","\n","\n","\n","def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","    \n","#     if not os.path.exists('')\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(sample_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                \n","                title = f'predicted: {class_names[preds[j]]}, label: {labels[j]}'\n","                imshow(inputs.cpu().data[j], title=title, fig_size=(12,7))\n","                \n","                if not os.path.exists(os.path.join('output', 'prediction_examples')):\n","                    os.mkdir(os.path.join('output', 'prediction_examples'))\n","                path = os.path.join('output', 'prediction_examples',f'test_image-{j}.jpg')\n","                plt.savefig(path)\n","                plt.show()\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T00:18:56.379014Z","iopub.status.busy":"2023-05-14T00:18:56.377820Z","iopub.status.idle":"2023-05-14T00:19:01.839125Z","shell.execute_reply":"2023-05-14T00:19:01.837988Z","shell.execute_reply.started":"2023-05-14T00:18:56.378964Z"},"trusted":true},"outputs":[],"source":["## Load Data\n","IM_SIZE = 640\n","batch_size = 16\n","# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize(IM_SIZE),\n","        transforms.CenterCrop(IM_SIZE),\n","        # transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(IM_SIZE),\n","        transforms.CenterCrop(IM_SIZE),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(IM_SIZE),\n","        transforms.CenterCrop(IM_SIZE),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","}\n","\n","DATA_DIR = 'DB'\n","ds_splits = ['train', 'val', 'test']\n","image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x),\n","                                          data_transforms[x])\n","                  for x in ds_splits}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n","                                             shuffle=True, num_workers=2)\n","              for x in ds_splits[:2]}\n","\n","sample_loader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size,\n","                                        shuffle=True, num_workers=2)\n","\n","test_dataloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size,\n","                                        shuffle=False, num_workers=2)\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ds_splits}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(f'Using device {device}')\n","\n","def imshow(inp, title=None, fig_size=(12,7)):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.figure(figsize=fig_size)\n","#     plt.savefig('batch.jpg')\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out, title=[class_names[x] for x in classes], fig_size=(20,15))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T06:16:56.957271Z","iopub.status.busy":"2023-05-14T06:16:56.956837Z","iopub.status.idle":"2023-05-14T06:16:57.583214Z","shell.execute_reply":"2023-05-14T06:16:57.582206Z","shell.execute_reply.started":"2023-05-14T06:16:56.957220Z"},"id":"c5pq-NMNlKQz","trusted":true},"outputs":[],"source":["import datetime\n","\n","TIME_STAMP = datetime.datetime.today().strftime('%d_%h_%y_%H_%M')\n","model_weights = models.ResNet50_Weights.DEFAULT\n","\n","model_ft = models.resnet50(weights=model_weights)\n","for param in model_ft.parameters():\n","    param.requires_grad = False\n","num_ftrs = model_ft.fc.in_features\n","model_ft.fc = nn.Linear(num_ftrs, 5)\n","model_ft = nn.DataParallel(model_ft)\n","model_ft = model_ft.to(device)\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer_ft = optim.SGD(\n","    model_ft.parameters(), \n","    lr=0.01,\n","    weight_decay=1e-5, \n","    momentum=0.9\n",")\n","\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n","model_ft"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T06:17:08.037044Z","iopub.status.busy":"2023-05-14T06:17:08.036293Z","iopub.status.idle":"2023-05-14T06:52:25.135940Z","shell.execute_reply":"2023-05-14T06:52:25.134747Z","shell.execute_reply.started":"2023-05-14T06:17:08.037002Z"},"id":"NZ8j8uABgcUY","outputId":"16656ddd-9abd-4dd8-a4bf-bd33e8ea5337","trusted":true},"outputs":[],"source":["warmup_epochs = 7\n","training_epochs = 25\n","\n","print(f\"Training on device {device}\")\n","# for param in model_ft.parameters():\n","#     param.requires_grad = False\n","\n","best_train_loss, model_ft = train_model(\n","    model = model_ft, \n","    criterion = criterion, \n","    optimizer = optimizer_ft, \n","    scheduler = exp_lr_scheduler,\n","    num_epochs = training_epochs,\n","    warmup_epochs = warmup_epochs\n","    \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T05:15:02.035223Z","iopub.status.busy":"2023-05-14T05:15:02.034761Z","iopub.status.idle":"2023-05-14T05:15:02.289789Z","shell.execute_reply":"2023-05-14T05:15:02.288725Z","shell.execute_reply.started":"2023-05-14T05:15:02.035178Z"},"id":"3iBWIPTEbVCx","trusted":true},"outputs":[],"source":["if not os.path.exists('models'):\n","  os.mkdir('models')\n","\n","FILENAME = f\"{model_weights}_{warmup_epochs}_{training_epochs}_{best_train_loss}_Grading_{TIME_STAMP}\"\n","print(f'{FILENAME}')\n","MODEL_PATH = f'{FILENAME}.pt'\n","torch.save(model_ft.state_dict(), MODEL_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-14T06:52:32.110007Z","iopub.status.busy":"2023-05-14T06:52:32.109254Z","iopub.status.idle":"2023-05-14T06:52:58.980159Z","shell.execute_reply":"2023-05-14T06:52:58.978957Z","shell.execute_reply.started":"2023-05-14T06:52:32.109962Z"},"id":"VFgnK8QMlE5a","trusted":true},"outputs":[],"source":["y_pred = []\n","y_true = []\n","# iterate over test data\n","print(\"[INFO] Iterating predictions\")\n","\n","# device = torch.device(\"cpu\")\n","# model_ft = model_ft.to(device)\n","model_ft.eval()\n","\n","for inputs, labels in test_dataloader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        output = model_ft(inputs) # Feed Network\n","\n","        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n","        y_pred.extend(output) # Save Prediction\n","        \n","        labels = labels.data.cpu().numpy()\n","        y_true.extend(labels) # Save Truth\n","                \n","print(\"[INFO] Finished iterating predictions\")\n","\n","# constant for classes\n","classes = ('No DR', 'Mild NPDR', 'Moderate NPDR', 'Severe NPDR', 'Proliferative DR')\n","\n","def get_matrix(cfm: np.ndarray, pct:True):\n","        if pct:\n","                return cfm / np.sum(cfm, axis=1)\n","        else:\n","                return cfm\n","\n","if not os.path.exists('output'):\n","  os.mkdir('output')\n","OUTPATH = os.path.sep.join(['output',f'{FILENAME}'])\n","if not os.path.exists(OUTPATH):\n","        os.mkdir(OUTPATH)\n","\n","visualize_model(model_ft)\n","print(\"[INFO] Building Matrix\")\n","# Build confusion matrix\n","cf_matrix = confusion_matrix(y_true, y_pred)\n","for pct in (False, True):\n","        df_cm = pd.DataFrame(get_matrix(cfm=cf_matrix,pct=pct), index = [i for i in classes],\n","                        columns = [i for i in classes])\n","        plt.figure(figsize = (12,7))\n","        sn.heatmap(df_cm, annot=True, fmt='g')\n","        cm_path = os.path.sep.join([f'confusion_matrix_pct_{pct}.png'])\n","        plt.savefig(cm_path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
