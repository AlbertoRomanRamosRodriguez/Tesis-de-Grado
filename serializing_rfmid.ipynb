{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tesis_lib.preprocessing.aspectawareprocessor import AspectAwareProcessor\n",
    "from tesis_lib.io.hdf5datasetwriter import HDF5DatasetWriter\n",
    "\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import progressbar\n",
    "import json\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './DB'\n",
    "\n",
    "IM_SIZE = 256\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_HDF5_PATH = os.path.sep.join([DATASET_PATH, 'hdf5'])\n",
    "if os.path.exists(DATASET_HDF5_PATH):\n",
    "    !rm -r {DATASET_HDF5_PATH}\n",
    "\n",
    "os.mkdir(DATASET_HDF5_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aap = AspectAwareProcessor(IM_SIZE,IM_SIZE)\n",
    "# iap = ImageToArrayPreprocessor()\n",
    "(R,G,B) = ([],[],[])\n",
    "\n",
    "path = os.path.sep.join([DATASET_PATH, \"Training\"])\n",
    "class_paths = [os.path.sep.join([path, im_class]) for im_class in os.listdir(path)]\n",
    "\n",
    "imagePaths = []\n",
    "[imagePaths.extend(paths.list_images(cp)) for cp in class_paths]\n",
    "labels = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "(trainPaths, valPaths,trainLabels,valLabels) = train_test_split(\n",
    "  imagePaths,\n",
    "  labels,\n",
    "  train_size=450,\n",
    "  test_size=100,\n",
    "  stratify=labels, \n",
    "  random_state = 42)\n",
    "\n",
    "assert trainLabels.shape[0] == len(trainPaths)\n",
    "assert valLabels.shape[0] == len(valPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points = 450\n",
      "Training data points = 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data points = {trainLabels.shape[0]}\")\n",
    "print(f\"Training data points = {valLabels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.sep.join([DATASET_PATH, \"Test\"])\n",
    "class_paths = [os.path.sep.join([path, im_class]) for im_class in os.listdir(path)]\n",
    "\n",
    "imagePaths = []\n",
    "[imagePaths.extend(paths.list_images(cp)) for cp in class_paths]\n",
    "labels = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "\n",
    "(_, testPaths,_,testLabels) = train_test_split(\n",
    "  imagePaths,\n",
    "  labels,\n",
    "  train_size=450,\n",
    "  test_size=124,\n",
    "  stratify=labels, \n",
    "  random_state = 42)\n",
    "\n",
    "assert testLabels.shape[0] == len(testPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data points = 124\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data points = {testLabels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOoUuQ4E_2_N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building train Set: 100% |######################################| Time: 0:00:29\n",
      "Building val Set: 100% |########################################| Time: 0:00:08\n",
      "Building test Set: 100% |#######################################| Time: 0:00:11\n"
     ]
    }
   ],
   "source": [
    "DATA_PATHS = [\n",
    "    ('train', trainPaths, trainLabels, f'./DB/hdf5/Training.hdf5'),\n",
    "    ('val', valPaths, valLabels, f'./DB/hdf5/Validation.hdf5'),\n",
    "    ('test', testPaths, testLabels, f'./DB/hdf5/Testing.hdf5'),\n",
    "]\n",
    "\n",
    "for (dType, imagePaths, labels, output) in DATA_PATHS:\n",
    "  if os.path.exists(output):\n",
    "    os.remove(output)\n",
    "  writer = HDF5DatasetWriter((len(imagePaths), IM_SIZE,IM_SIZE,3), output)\n",
    "\n",
    "  widgets = [\n",
    "      f\"Building {dType} Set: \",\n",
    "      progressbar.Percentage(),\n",
    "      \" \",\n",
    "      progressbar.Bar(),\n",
    "      \" \",\n",
    "      progressbar.ETA()\n",
    "  ]\n",
    "\n",
    "  pbar = progressbar.ProgressBar(\n",
    "      maxval=len(imagePaths),\n",
    "      widgets=widgets\n",
    "      ).start()\n",
    "\n",
    "  for (i, (path,label)) in enumerate(zip(imagePaths, labels)):\n",
    "      image = cv2.imread(path)\n",
    "      try:\n",
    "        image = aap.preprocess(image)\n",
    "      except Exception:\n",
    "        display(f\"[WARNING] Skipped {path.split('/')[-1]}\")\n",
    "        continue\n",
    "\n",
    "      if dType == \"train\":\n",
    "        (b,g,r) = cv2.mean(image)[:3]\n",
    "        R.append(r)\n",
    "        G.append(g)\n",
    "        B.append(b)\n",
    "      \n",
    "      writer.add([image], [label])\n",
    "      pbar.update(i)\n",
    "\n",
    "  pbar.finish()\n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing means...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] serializing means...\")\n",
    "D = {\n",
    "    \"R\": np.mean(R),\n",
    "    \"G\": np.mean(G),\n",
    "    \"B\": np.mean(B)\n",
    "}\n",
    "with open('./DB/hdf5/diat_ret.json', \"w\") as f:\n",
    "    f.write(json.dumps(D))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesisDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66e04bfc85808da2e18bbbef2b64a6be2d087de19c3aa024c126ff1bcb5631ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
